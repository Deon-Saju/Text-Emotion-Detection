# -*- coding: utf-8 -*-
"""Fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yx5wJBQsrQRMS3aoXIo8SYgGHz1GgmVn
"""

# Install libs (Colab)
!pip install -q -U transformers datasets evaluate

from transformers import TrainingArguments
print(TrainingArguments)

# Imports & seed
import random
import numpy as np
import torch

RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_SEED)

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding
import evaluate
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

# Charge dataset emotion (Hugging Face)
dataset = load_dataset("emotion")
dataset

# Inspect labels
label_names = dataset["train"].features["label"].names
print("Labels:", label_names)
print("Exemple:", dataset["train"][0])

# Choose the model
MODEL_NAME = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Tokenization
def preprocess(batch):
    return tokenizer(batch["text"], truncation=True)

tokenized = dataset.map(preprocess, batched=True)
tokenized = tokenized.remove_columns(["text"])  # on garde tokens + label
tokenized.set_format("torch")
tokenized

# Data collator & model init
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
num_labels = len(label_names)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)

# Metrics (accuracy + macro F1)
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy.compute(predictions=preds, references=labels)["accuracy"]
    f1m = f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    return {"accuracy": acc, "f1_macro": f1m}

# Training args
training_args = TrainingArguments(
    output_dir="emotion_model",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    seed=RANDOM_SEED,
    logging_steps=100,
)

# Subset option

USE_SUBSET = False
if USE_SUBSET:
    small_train = tokenized["train"].shuffle(seed=RANDOM_SEED).select(range(2000))
    small_val   = tokenized["validation"].shuffle(seed=RANDOM_SEED).select(range(500))
    small_test  = tokenized["test"].shuffle(seed=RANDOM_SEED).select(range(500))
else:
    small_train = tokenized["train"]
    small_val   = tokenized["validation"]
    small_test  = tokenized["test"]

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train,
    eval_dataset=small_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train
trainer.train()

# Evaluate on test set
metrics = trainer.evaluate(eval_dataset=small_test)
print(metrics)

# Predictions + classification report + confusion matrix
preds_output = trainer.predict(small_test)
preds = np.argmax(preds_output.predictions, axis=-1)
labels = preds_output.label_ids

print(classification_report(labels, preds, target_names=label_names))

cm = confusion_matrix(labels, preds)
print("Confusion matrix:\n", cm)

# Plot CM
plt.figure(figsize=(7,6))
plt.imshow(cm, interpolation='nearest')
plt.title("Matrice de confusion")
plt.xticks(range(len(label_names)), label_names, rotation=45)
plt.yticks(range(len(label_names)), label_names)
plt.xlabel("Predicted")
plt.ylabel("True")
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha="center", va="center")
plt.tight_layout()
plt.show()

from pathlib import Path

# Set a folder name inside Google Drive
save_dir = "/content/drive/MyDrive/emotion_model_saved"

# Create folder if it doesn’t exist
Path(save_dir).mkdir(parents=True, exist_ok=True)

# Save model and tokenizer
trainer.save_model(save_dir)
tokenizer.save_pretrained(save_dir)

print("Model & tokenizer saved to:", save_dir)

from transformers import pipeline

model_path = "/content/drive/MyDrive/emotion_model_saved"

emotion_classifier = pipeline(
    "text-classification",
    model=model_path,
    tokenizer=model_path,
    return_all_scores=True
)

# Label mapping
label_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]

# Example texts
texts = [
    "I’m feeling amazing today!",
    "I hate when people lie.",
    "I miss my friends so much.",
    "You make me feel loved ❤️",
    "That movie scared me a lot!"
]

# Run predictions
for t in texts:
    preds = emotion_classifier(t)[0]
    preds_sorted = sorted(preds, key=lambda x: x["score"], reverse=True)
    top_label = preds_sorted[0]["label"]
    label_id = int(top_label.split("_")[-1])
    print(f"\nText: {t}")
    print(f"→ Top emotion: {label_names[label_id]} ({preds_sorted[0]['score']:.2f})")