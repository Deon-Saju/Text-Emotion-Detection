# -*- coding: utf-8 -*-
"""Emotion Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ff-mKa2rt1YznEKxxXjHzyhyWlPmmMpw
"""

# Install libs (Colab)
!pip install -q -U transformers datasets evaluate

from transformers import TrainingArguments
print(TrainingArguments)

# Imports & seed
import random
import numpy as np
import torch

RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_SEED)

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding
import evaluate
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

# Charge dataset emotion (Hugging Face)
dataset = load_dataset("emotion")
dataset
# structure: train/validation/test with 'text' and 'label'

# Inspect labels
label_names = dataset["train"].features["label"].names
print("Labels:", label_names)
print("Exemple:", dataset["train"][0])

# Choose the model
MODEL_NAME = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Tokenization
def preprocess(batch):
    return tokenizer(batch["text"], truncation=True)

tokenized = dataset.map(preprocess, batched=True)
tokenized = tokenized.remove_columns(["text"])  # on garde tokens + label
tokenized.set_format("torch")
tokenized

# Data collator & model init
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
num_labels = len(label_names)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)

# Metrics (accuracy + macro F1)
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy.compute(predictions=preds, references=labels)["accuracy"]
    f1m = f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    return {"accuracy": acc, "f1_macro": f1m}

# Training args
training_args = TrainingArguments(
    output_dir="emotion_model",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    seed=RANDOM_SEED,
    logging_steps=100,
)

# Subset option

USE_SUBSET = False
if USE_SUBSET:
    small_train = tokenized["train"].shuffle(seed=RANDOM_SEED).select(range(2000))
    small_val   = tokenized["validation"].shuffle(seed=RANDOM_SEED).select(range(500))
    small_test  = tokenized["test"].shuffle(seed=RANDOM_SEED).select(range(500))
else:
    small_train = tokenized["train"]
    small_val   = tokenized["validation"]
    small_test  = tokenized["test"]

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train,
    eval_dataset=small_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train
trainer.train()

# Evaluate on test set
metrics = trainer.evaluate(eval_dataset=small_test)
print(metrics)

# Predictions + classification report + confusion matrix
preds_output = trainer.predict(small_test)
preds = np.argmax(preds_output.predictions, axis=-1)
labels = preds_output.label_ids

print(classification_report(labels, preds, target_names=label_names))

cm = confusion_matrix(labels, preds)
print("Confusion matrix:\n", cm)

# Plot CM
plt.figure(figsize=(7,6))
plt.imshow(cm, interpolation='nearest')
plt.title("Matrice de confusion")
plt.xticks(range(len(label_names)), label_names, rotation=45)
plt.yticks(range(len(label_names)), label_names)
plt.xlabel("Predicted")
plt.ylabel("True")
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha="center", va="center")
plt.tight_layout()
plt.show()

from google.colab import drive
drive.mount('/content/drive')

# Save model & tokenizer (option: Google Drive)
from pathlib import Path
save_dir = "emotion_model_saved"
trainer.save_model(save_dir)
tokenizer.save_pretrained(save_dir)
print("Saved to", save_dir)

"""## Predict emotions for custom sentences:"""

# Rebuild pipeline
from transformers import pipeline

model_path = "emotion_model_saved"

# Load model + tokenizer again
emotion_classifier = pipeline(
    "text-classification",
    model=model_path,
    tokenizer=model_path,
    return_all_scores=True
)

# Label mapping (from dataset)
label_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]

# Now predict properly
texts = [
    "I‚Äôm feeling amazing today!",
    "I hate when people lie.",
    "I miss my friends so much.",
    "You make me feel loved ‚ù§Ô∏è",
    "That movie scared me a lot!"
]

for t in texts:
    preds = emotion_classifier(t)[0]
    preds_sorted = sorted(preds, key=lambda x: x["score"], reverse=True)
    top_label = preds_sorted[0]["label"]
    label_id = int(top_label.split("_")[-1])  # extract number from LABEL_*
    print(f"\nText: {t}")
    print(f"‚Üí Top emotion: {label_names[label_id]} ({preds_sorted[0]['score']:.2f})")

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

model_path = "./emotion_model_saved"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path, trust_remote_code=True)

emotion_classifier = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    return_all_scores=True
)

print(emotion_classifier("I am so happy today!"))

"""## Gradio App"""

!pip install -q gradio transformers torch

# --- Imports
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import gradio as gr
from pathlib import Path

# --- Load model directly (since it's local)
model_path = Path("emotion_model_saved")  # your folder in /content

# Load tokenizer and model from that folder
tokenizer = AutoTokenizer.from_pretrained(str(model_path))
model = AutoModelForSequenceClassification.from_pretrained(str(model_path))

# Build pipeline using the actual model objects (avoids HFValidationError)
emotion_classifier = pipeline(
    task="text-classification",
    model=model,
    tokenizer=tokenizer,
    return_all_scores=True
)

# --- Label mapping
label_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]

# --- Prediction function
def predict_emotion(text):
    preds = emotion_classifier(text)[0]
    preds_sorted = sorted(preds, key=lambda x: x["score"], reverse=True)
    results = {
        label_names[int(p["label"].split("_")[-1])]: float(p["score"])
        for p in preds_sorted
    }
    return results

# --- Gradio interface
demo = gr.Interface(
    fn=predict_emotion,
    inputs=gr.Textbox(label="üí¨ Enter a sentence or tweet"),
    outputs=gr.Label(num_top_classes=3, label="Predicted Emotions"),
    title="Emotion Detector ü§ñüí≠",
    description="Type any text to detect emotions using your fine-tuned model!",
    theme="soft",
    examples=[
        ["I‚Äôm feeling amazing today!"],
        ["I hate when people lie."],
        ["You make me feel loved ‚ù§Ô∏è"],
        ["That movie scared me a lot!"]
    ]
)

demo.launch(share=True)

